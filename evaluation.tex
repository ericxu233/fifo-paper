\section{Evaluation}
\label{sec:evaluation}

To evaluate FIFO initialization and the associated optimizations, we implemented a cycle-accurate simulator of our CGRA architecture with FIFO initialization support. We compared the performance of our approach against a baseline CGRA architecture without FIFO initialization across a suite of benchmarks with varying loop-carried dependence characteristics.

\subsection{Experimental Setup}
\label{subsec:exp-setup}
Our simulator models a CGRA with 8x8 processing elements (PEs) with IO/Memory PEs along the left and right edges which gives 16 nodes for memory io and 48 nodes for compute. Each PE is equipped with a FIFO of depth 8 by default. The latency of floating-point PE operations is described in Table~\ref{tab:pe-latency}. Other operations such as integer arithmetic and logic operations are set to have a latency of 1 cycle. The inter-PE communication latency is set to 1 cycle. 

Our CGRA assumes an elastic dataflow execution model along with data streaming. Data is streamed onto and out of the CGRA from DRAM at a constant rate of 16 words per cycle. The latency to access DRAM is set to 400 cycles by default which dictates the initial latency to start streaming data as well as the stall cycles when FIFOs fill up due to pipeline imbalance.

To model the runtime overhead of the peeled-loop iterations introduced by the optimizations in Section \ref{sec:optimization}, we assume that each peeled iteration incurs an additional overhead of 3 cyles times the number of operations in the target loop body.

\begin{table}[h]
\centering
\caption{PE Operation Latencies (in cycles)}
\label{tab:pe-latency}
\begin{tabular}{|l|c|}
\hline
\textbf{Operation} & \textbf{Latency} \\
\hline
fadd & 3 \\
fsub & 3 \\
fmul & 4 \\
fdiv & 6 \\
fcomp & 2 \\
exp & 6 \\
\hline
\end{tabular}
\end{table}

The baseline CGRA architecture is identical except that it does not support FIFO initialization. Therefore, loop-carried dependences must be handled using the PHI node approach.

The benchmark suite consists of various benchmarks found in scientific computing, machine learning, and signal processing domains. We take some of the benchmarks from Polybench \cite{pouchet2012polybench} and some from prior works on accelerating linear recurrences \cite{maleki2018automatic}. Table~\ref{tab:benchmarks} summarizes the benchmarks used in our evaluation.

\begin{table}[h]
\centering
\caption{Benchmark Suite}
\label{tab:benchmarks}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Benchmark} & \textbf{Category} & \textbf{Description} \\
\hline
\multicolumn{3}{|c|}{\textit{Linear Recurrences}} \\
\hline
ema & Signal Proc. & Exponential moving average \\
exp\_decay & Signal Proc. & Exponential decay filter \\
hp\_filter & Signal Proc. & High-pass filter \\
momentum\_sgd & ML & SGD with momentum update \\
weighted\_avg & Signal Proc. & Weighted sum recurrence \\
\hline
\multicolumn{3}{|c|}{\textit{Reductions}} \\
\hline
gemm & Linear Algebra & Matrix multiplication \\
mvt & Linear Algebra & Matrix-vector product \\
gaussian\_cdf & Statistics & Gaussian CDF computation \\
sep & Physics & Spring energy profile \\
fd & Physics & Fluid dynamics simulation \\
\hline
\multicolumn{3}{|c|}{\textit{Stencils}} \\
\hline
conv2d & Image Proc. & 3$\times$3 2D convolution \\
conv5x5 & Image Proc. & 5$\times$5 2D convolution \\
jacobi2d & Numerical & Jacobi 2D stencil \\
\hline
\end{tabular}
\end{table}

\subsection{Reductions and Linear Recurrences}
\label{subsec:overall-speedup}

% Load pre-processed data from artifacts (generated by scripts/generate_plot_data.py)
\pgfplotstableread[col sep=comma]{artifacts/throughput_comparison.csv}\throughputdata

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar=0pt,
    width=\columnwidth,
    height=6cm,
    bar width=6pt,
    ylabel={Throughput (iter/cycle)},
    symbolic x coords={ema, exp_decay, hp_filter, momentum_sgd, weighted_avg, gaussian_cdf, gemm, mvt, sep, fd},
    xtick=data,
    xticklabels={ema, exp\_decay, hp\_filter, momentum\_sgd, weighted\_avg, gaussian\_cdf, gemm, mvt, sep, fd},
    x tick label style={rotate=45, anchor=east, font=\footnotesize},
    ymin=0,
    ymax=1.1,
    ytick={0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0},
    yticklabel style={font=\footnotesize},
    legend style={at={(0.5,1.02)}, anchor=south, legend columns=3, font=\scriptsize},
    legend cell align={left},
    enlarge x limits=0.08,
]

% Baseline (PHI-only)
\addplot[fill=gray!60, draw=none] table[x=testname, y=baseline] {\throughputdata};

% FIFO Initialization
\addplot[fill=blue!60, draw=none] table[x=testname, y=fifo_init] {\throughputdata};

% FIFO Init + Lookahead
\addplot[fill=red!60, draw=none] table[x=testname, y=lookahead] {\throughputdata};

\legend{Baseline (PHI-only), FIFO Init, FIFO Init + Lookahead}
\end{axis}
\end{tikzpicture}
\caption{Throughput comparison across benchmarks for reductions and linear recurrences}
\label{fig:throughput-comparison}
\end{figure}

Figure~\ref{fig:throughput-comparison} presents the throughput comparison across our benchmark suite for reductions and linear recurrences. We compare three configurations: (1) the baseline PHI-only implementation without FIFO initialization, (2) FIFO initialization without lookahead expansion, and (3) FIFO initialization with lookahead expansion enabled.

The results demonstrate that FIFO initialization alone provides modest throughput improvements, ranging from 1.1$\times$ to 1.4$\times$ over the baseline. This improvement comes from eliminating the PHI node and reducing the feedback-cycle latency. However, the most significant gains are achieved when combining FIFO initialization with lookahead expansion, which enables effective pipelining of the loop-carried dependence chain. With lookahead expansion, throughput improves dramatically---up to 12.7$\times$ and over 9$\times$ on average across all benchmarks. It shows that look-ahead expansion effectively leverages FIFO initialization and completely eliminates the feedback-cycle bottleneck to achieve the desired throughput.

\subsection{Impact of Streaming Latency}
\label{subsec:streaming-latency}

Figure~\ref{fig:streaming-latency-impact} shows the impact of varying streaming latency on the speedup achieved over the baseline for selected benchmarks. We vary the streaming latency from 0 to 600 cycles while keeping the FIFO size fixed at 8 entries. The results demonstrate that as streaming latency increases, the relative speedup of our approach generally increases. This is because higher streaming latency leads to more stall cycles in the baseline due to pipeline imbalance, while our lookahead expansion along with pipeline balancing technique effectively eliminates streaming stalls. We can see there are two distinct groups of benchmarks at streaming latency of 0. They represent linear recurrences and reductions which all display a look-ahead speedup of $10\times$ and $6\times$. These numbers represent the inherent benefits of lookahead expansion that compsensates for the feedback latency bottleneck wihtout the influence of streaming stalls. The benchmarks with longer recurrence chains (e.g., \texttt{ema}, \texttt{exp\_decay}) and complex graph structures (e.g., \texttt{fdf}) show the most dramatic improvements as streaming latency increases, achieving over 13$\times$ speedup at high latencies.

% Load streaming latency impact data (generated by scripts/generate_plot_data.py)
\pgfplotstableread[col sep=comma]{artifacts/streaming_latency_impact.csv}\streaminglatencydata

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=\columnwidth,
    height=6cm,
    xlabel={Streaming Latency (cycles)},
    ylabel={Speedup over Baseline},
    xmin=-30,
    xmax=630,
    xtick={0, 100, 200, 300, 400, 500, 600},
    ymin=0,
    ymax=15,
    ytick={0, 2, 4, 6, 8, 10, 12, 14},
    grid=major,
    grid style={dashed, gray!30},
    legend style={at={(0.5,1.02)}, anchor=south, legend columns=4, font=\scriptsize},
    legend cell align={left},
]

% ema
\addplot[color=blue, mark=*, line width=1pt, mark size=2pt] 
    table[x=streaming_latency, y=ema] {\streaminglatencydata};

% exp_decay
\addplot[color=red, mark=square*, line width=1pt, mark size=2pt] 
    table[x=streaming_latency, y=exp_decay] {\streaminglatencydata};

% hp_filter
\addplot[color=green!60!black, mark=triangle*, line width=1pt, mark size=2pt] 
    table[x=streaming_latency, y=hp_filter] {\streaminglatencydata};

% momentum_sgd
\addplot[color=orange, mark=diamond*, line width=1pt, mark size=2pt] 
    table[x=streaming_latency, y=momentum_sgd] {\streaminglatencydata};

% weighted_avg
\addplot[color=purple, mark=pentagon*, line width=1pt, mark size=2pt] 
    table[x=streaming_latency, y=weighted_avg] {\streaminglatencydata};

% gaussian_cdf
\addplot[color=cyan, mark=otimes*, line width=1pt, mark size=2pt] 
    table[x=streaming_latency, y=gaussian_cdf] {\streaminglatencydata};

% gemm
\addplot[color=brown, mark=star, line width=1pt, mark size=2.5pt] 
    table[x=streaming_latency, y=gemm] {\streaminglatencydata};

% mvt
\addplot[color=magenta, mark=oplus*, line width=1pt, mark size=2pt] 
    table[x=streaming_latency, y=mvt] {\streaminglatencydata};

% sep
\addplot[color=gray, mark=x, line width=1pt, mark size=2.5pt] 
    table[x=streaming_latency, y=sep] {\streaminglatencydata};

% fd
\addplot[color=teal, mark=+, line width=1pt, mark size=2.5pt] 
    table[x=streaming_latency, y=fd] {\streaminglatencydata};

\legend{ema, exp\_decay, hp\_filter, momentum\_sgd, weighted\_avg, gaussian\_cdf, gemm, mvt, sep, fd}
\end{axis}
\end{tikzpicture}
\caption{Impact of streaming latency on speedup over baseline for selected benchmarks}
\label{fig:streaming-latency-impact}
\end{figure}

\subsection{Impact of FIFO Size}
\label{subsec:fifo-size}

Figure~\ref{fig:fifo-size-impact} illustrates the impact of varying FIFO sizes on the speedup achieved over the baseline for selected benchmarks. We vary the FIFO size from 4 to 16 entries while keeping other parameters constant. It is illustrated that there is a performance gain when FIFO size jumps from 4 to 6 entries for most benchmarks. This is because a FIFO size of 4 is often insufficient to fully buffer the data required for lookahead expansion, leading to multiple buffer PE used which increase the feedback latency. However, going beyond 8 shows degrading or flat speedup for most benchmarks. This is because larger FIFO sizes can reduce the number of stalls due to pipeline imbalance and therefore the baseline performance improves, which in turn reduces the relative speedup achieved by our optimizations.

% Load FIFO size impact data (generated by scripts/generate_plot_data.py)
\pgfplotstableread[col sep=comma]{artifacts/fifo_size_impact.csv}\fifosizedata

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    width=\columnwidth,
    height=6cm,
    xlabel={FIFO Size},
    ylabel={Speedup over Baseline},
    xmin=3,
    xmax=17,
    xtick={4, 6, 8, 10, 12, 14, 16},
    ymin=0,
    ymax=15,
    ytick={0, 2, 4, 6, 8, 10, 12, 14},
    grid=major,
    grid style={dashed, gray!30},
    legend style={at={(0.5,1.02)}, anchor=south, legend columns=4, font=\scriptsize},
    legend cell align={left},
]

% ema
\addplot[color=blue, mark=*, line width=1pt, mark size=2pt] 
    table[x=fifo_size, y=ema] {\fifosizedata};

% exp_decay
\addplot[color=red, mark=square*, line width=1pt, mark size=2pt] 
    table[x=fifo_size, y=exp_decay] {\fifosizedata};

% hp_filter
\addplot[color=green!60!black, mark=triangle*, line width=1pt, mark size=2pt] 
    table[x=fifo_size, y=hp_filter] {\fifosizedata};

% momentum_sgd
\addplot[color=orange, mark=diamond*, line width=1pt, mark size=2pt] 
    table[x=fifo_size, y=momentum_sgd] {\fifosizedata};

% weighted_avg
\addplot[color=purple, mark=pentagon*, line width=1pt, mark size=2pt] 
    table[x=fifo_size, y=weighted_avg] {\fifosizedata};

% gaussian_cdf
\addplot[color=cyan, mark=otimes*, line width=1pt, mark size=2pt] 
    table[x=fifo_size, y=gaussian_cdf] {\fifosizedata};

% gemm
\addplot[color=brown, mark=star, line width=1pt, mark size=2.5pt] 
    table[x=fifo_size, y=gemm] {\fifosizedata};

% mvt
\addplot[color=magenta, mark=oplus*, line width=1pt, mark size=2pt] 
    table[x=fifo_size, y=mvt] {\fifosizedata};

% sep
\addplot[color=gray, mark=x, line width=1pt, mark size=2.5pt] 
    table[x=fifo_size, y=sep] {\fifosizedata};

% fd
\addplot[color=teal, mark=+, line width=1pt, mark size=2.5pt] 
    table[x=fifo_size, y=fd] {\fifosizedata};

\legend{ema, exp\_decay, hp\_filter, momentum\_sgd, weighted\_avg, gaussian\_cdf, gemm, mvt, sep, fd}
\end{axis}
\end{tikzpicture}
\caption{Impact of FIFO size on speedup over baseline for selected benchmarks}
\label{fig:fifo-size-impact}
\end{figure}

\subsection{Comparison with Parallel Loop Unrolling}
\label{subsec:parallel-unrolling}

Figure~\ref{fig:unroll-comparison} compares throughput across four configurations: (1) baseline without any optimizations, (2) parallel loop unrolling without lookahead, (3) lookahead expansion without unrolling, and (4) lookahead expansion combined with parallel loop unrolling. For the unrolled configurations, we select the maximum unroll factor that keeps the dataflow graph size under 64 nodes and with memory nodes constrained to under 16 nodes and compute nodes constrined under 48 nodes to fit wihtin the CGRA architecture's limit.

The results show that parallel loop unrolling alone provides substantial throughput improvements over the baseline, achieving up to 9.9$\times$ improvement for reduction benchmarks like \texttt{gemm} and \texttt{mvt}. However, lookahead expansion without unrolling achieves competitive or better throughput for most benchmarks while using significantly fewer PEs. Most importantly, combining lookahead expansion with parallel unrolling achieves the highest throughput across all benchmarks. This demonstrates that these optimizations are complementary. The combined approach achieves 4.04 $\times$ average speedup over unrolling alone and 2.84 $\times$ average speedup over lookahead alone, substantially outperforming either optimization in isolation.

% Load unroll comparison data (generated by scripts/generate_plot_data.py)
\pgfplotstableread[col sep=comma]{artifacts/unroll_comparison.csv}\unrolldata

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar=0pt,
    width=\columnwidth,
    height=6cm,
    bar width=5pt,
    ylabel={Throughput (iter/cycle)},
    symbolic x coords={ema, exp_decay, hp_filter, momentum_sgd, weighted_avg, gaussian_cdf, gemm, mvt, sep},
    xtick=data,
    xticklabels={ema, exp\_decay, hp\_filter, momentum\_sgd, weighted\_avg, gaussian\_cdf, gemm, mvt, sep},
    x tick label style={rotate=45, anchor=east, font=\footnotesize},
    ymin=0,
    ymax=5,
    ytick={0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0},
    yticklabel style={font=\footnotesize},
    legend style={at={(0.5,1.02)}, anchor=south, legend columns=2, font=\scriptsize},
    legend cell align={left},
    enlarge x limits=0.08,
]

% Baseline
\addplot[fill=gray!60, draw=none] table[x=testname, y=baseline] {\unrolldata};

% Unrolled (no lookahead)
\addplot[fill=blue!60, draw=none] table[x=testname, y=unrolled] {\unrolldata};

% Lookahead (no unroll)
\addplot[fill=orange!60, draw=none] table[x=testname, y=lookahead] {\unrolldata};

% Unrolled + Lookahead
\addplot[fill=red!60, draw=none] table[x=testname, y=unrolled_lookahead] {\unrolldata};

\legend{Baseline, Unrolled, Lookahead, Unrolled + Lookahead}
\end{axis}
\end{tikzpicture}
\caption{Throughput comparison: baseline vs. parallel unrolling vs. lookahead vs. combined approach. Unroll factors selected to keep DFG size $<$ 64 nodes.}
\label{fig:unroll-comparison}
\end{figure}

Figure~\ref{fig:unroll-per-pe-comparison} presents the same comparison normalized by the number of PEs used (i.e., throughput per PE). This metric captures the efficiency of PE utilization and is important for understanding the resource-performance trade-off. The results reveal that lookahead expansion achieves significantly higher throughput per PE compared to parallel unrolling for most benchmarks, with an average speedup of 5.30$\times$ over unrolling alone and 1.38$\times$ over unrolling combined with lookahead. This is because lookahead expansion achieves high throughput with a logarithmic increase in PE count, while parallel unrolling increases PE usage linearly.

% Load unroll per-PE comparison data (generated by scripts/generate_plot_data.py)
\pgfplotstableread[col sep=comma]{artifacts/unroll_per_pe_comparison.csv}\unrollperpedata

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar=0pt,
    width=\columnwidth,
    height=6cm,
    bar width=5pt,
    ylabel={Throughput per PE (iter/cycle/PE)},
    symbolic x coords={ema, exp_decay, hp_filter, momentum_sgd, weighted_avg, gaussian_cdf, gemm, mvt, sep},
    xtick=data,
    xticklabels={ema, exp\_decay, hp\_filter, momentum\_sgd, weighted\_avg, gaussian\_cdf, gemm, mvt, sep},
    x tick label style={rotate=45, anchor=east, font=\footnotesize},
    ymin=0,
    ymax=0.16,
    ytick={0, 0.02, 0.04, 0.06, 0.08, 0.10, 0.12, 0.14, 0.16},
    yticklabel style={font=\footnotesize},
    legend style={at={(0.5,1.02)}, anchor=south, legend columns=2, font=\scriptsize},
    legend cell align={left},
    enlarge x limits=0.08,
]

% Baseline
\addplot[fill=gray!60, draw=none] table[x=testname, y=baseline] {\unrollperpedata};

% Unrolled (no lookahead)
\addplot[fill=blue!60, draw=none] table[x=testname, y=unrolled] {\unrollperpedata};

% Lookahead (no unroll)
\addplot[fill=orange!60, draw=none] table[x=testname, y=lookahead] {\unrollperpedata};

% Unrolled + Lookahead
\addplot[fill=red!60, draw=none] table[x=testname, y=unrolled_lookahead] {\unrollperpedata};

\legend{Baseline, Unrolled, Lookahead, Unrolled + Lookahead}
\end{axis}
\end{tikzpicture}
\caption{Throughput per PE comparison: demonstrates PE utilization efficiency across configurations.}
\label{fig:unroll-per-pe-comparison}
\end{figure}

\subsection{Impact of Pipeline Balancing}
\label{subsec:pipeline-balancing}

Figure~\ref{fig:pipeline-balancing-impact} illustrates the importance of pipeline balancing for achieving high throughput with lookahead expansion. We compare configurations with pipeline balancing enabled (PB=1) versus disabled (PB=0) across our benchmark suite using the maximum unroll factor that keeps the dataflow graph under 64 PEs. For the \texttt{fd} benchmark, we use the non-unrolled configuration from the streaming experiments.

The results show that pipeline balancing provides substantial speedups for most benchmarks. On average, enabling pipeline balancing achieves 4.22$\times$ speedup in throughput and 2.93$\times$ speedup in throughput per PE. The linear recurrence benchmarks (\texttt{ema}, \texttt{exp\_decay}, \texttt{hp\_filter}, \texttt{momentum\_sgd}, \texttt{weighted\_avg}) and reduction benchmarks (\texttt{gaussian\_cdf}, \texttt{sep}, \texttt{fd}) benefit significantly from pipeline balancing, with speedups ranging from 3.30$\times$ to 6.08$\times$. Notably, \texttt{gemm} and \texttt{mvt} show no improvement because these benchmarks have inherently balanced pipelines due to their simple reduction structure. The \texttt{exp\_decay} benchmark exhibits the highest speedup (6.08$\times$) due to its long recurrence chain that creates significant pipeline imbalance without the optimization.

% Load pipeline balancing impact data
\pgfplotstableread[col sep=comma]{artifacts/pipeline_balancing_impact.csv}\pbdata

\begin{figure}[t]
\centering
\begin{tikzpicture}
\begin{axis}[
    ybar=0pt,
    width=\columnwidth,
    height=6cm,
    bar width=8pt,
    ylabel={Speedup (PB on / PB off)},
    symbolic x coords={ema, exp_decay, hp_filter, momentum_sgd, weighted_avg, gaussian_cdf, gemm, mvt, sep, fd},
    xtick=data,
    xticklabels={ema (U4), exp\_decay (U4), hp\_filter (U3), momentum\_sgd (U2), weighted\_avg (U3), gaussian\_cdf (U4), gemm (U7), mvt (U7), sep (U6), fd},
    x tick label style={rotate=45, anchor=east, font=\footnotesize},
    ymin=0,
    ymax=7,
    ytick={0, 1, 2, 3, 4, 5, 6, 7},
    yticklabel style={font=\footnotesize},
    legend style={at={(0.5,1.02)}, anchor=south, legend columns=2, font=\scriptsize},
    legend cell align={left},
    enlarge x limits=0.08,
]

% Throughput speedup
\addplot[fill=blue!60, draw=none] table[x=testname, y=throughput_speedup] {\pbdata};

% Throughput per PE speedup
\addplot[fill=orange!60, draw=none] table[x=testname, y=throughput_per_pe_speedup] {\pbdata};

\legend{Throughput Speedup, Throughput per PE Speedup}
\end{axis}
\end{tikzpicture}
\caption{Impact of pipeline balancing: speedup achieved by enabling pipeline balancing (PB=1) over disabled (PB=0) for lookahead expansion with maximum unrolling under 64 PEs.}
\label{fig:pipeline-balancing-impact}
\end{figure}
